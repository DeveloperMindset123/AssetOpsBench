<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AgentWorkBench 2026</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; }
    h1, h2, h3 { color: #003366; }
    ul, ol { margin-left: 20px; }
    .authors, .section, .abstract, .topics, .committee { margin-bottom: 30px; }
    .small { font-size: 0.9em; color: #555; }
  </style>
</head>
<body>

<h1>AgentWorkBench 2026</h1>
<h2>Benchmarking AI Agents Across Modalities, Tasks, and Real-World Contexts</h2>

<div class="authors">
  <p><strong>Organizers:</strong><br>
  Ndivhuwo Makondo, Nianjun (Joe) Zhou, Karol Lynch (IBM Research)<br>
  Chathurangi Shyalika, Vedant Khandelwal (University of South Carolina)<br>
  Reza Nematirad (Kansas State University), Varsha Shukla (Dartmouth College)
  </p>
</div>

<div class="abstract">
  <h2>Abstract</h2>
  <p>This workshop centers on evaluating AI agents across modalities and tasks in dynamic real-world settings. We focus on trajectory-based behavior, multi-agent workflows, domain grounding, safety, memory, and regulatory compliance. Our aim is to unify the benchmarking community and drive forward robust, extensible frameworks for agent evaluation.</p>
</div>

<div class="section">
  <h2>Workshop Description and Relevance</h2>
  <p>Recent benchmarks like CVE-Bench, PaperBench, ST-WebAgentBench, and AssetOpsBench illustrate growing momentum for evaluating generative and agentic systems. This workshop responds to the gap between static benchmark tasks and complex agentic behaviors in real-world deployments, particularly in time-sensitive, policy-constrained, and industrial settings.</p>
</div>

<div class="topics">
  <h2>Topics of Interest</h2>
  <ul>
    <li>Reinforcement learning benchmarks (e.g., Atari, MuJoCo, MineRL)</li>
    <li>Standardized gym-style frameworks (e.g., OpenAI Gym)</li>
    <li>Code generation evaluation (e.g., HumanEval, APPS, ToolBench)</li>
    <li>Multimodal benchmarks (e.g., VQAv2, SEED-Bench)</li>
    <li>Time series agent evaluation (e.g., AssetOpsBench, ITBench)</li>
    <li>Agent planning and tool use (e.g., AgentBench, MIRAI, WebArena)</li>
    <li>Robustness and safety evaluation (e.g., HELM, AdversarialQA)</li>
    <li>Meta-evaluation frameworks (e.g., HELM, Open LLM Leaderboard)</li>
    <li>Advanced metrics: F1, BLEU, BERTScore, hallucination, latency</li>
    <li>Domain-specific benchmarks: finance, law, healthcare, industry</li>
    <li>Skill acquisition and generalization (e.g., ARC-style tasks)</li>
  </ul>
</div>

<div class="committee">
  <h2>Steering Committee</h2>
  <ul>
    <li>Amit Sheth (University of South Carolina)</li>
    <li>Jayant Kalagnanam (IBM)</li>
    <li>Dhaval Patel (IBM)</li>
    <li>Michal Shmueli-Scheuer (IBM)</li>
    <li>Utkarshani Jaimini (University of Michigan-Dearborn)</li>
    <li>Chinthaka Dinesh (Northeastern University)</li>
  </ul>
</div>

<div class="section">
  <h2>Invited Speakers</h2>
  <ul>
    <li>Prof. Mohan Kankanhalli (NUS)</li>
    <li>Jiatong Li (University of Wisconsin-Madison)</li>
    <li>Fanghua Ye (University College London)</li>
    <li>Ruoxi Sun &amp; Tao Yu (Google / HKU – SpiderSQL)</li>
    <li>Caiming Xiong (Salesforce Research)</li>
    <li>Kevin Chang (UIUC)</li>
  </ul>
</div>

<div class="section">
  <h2>Workshop Format</h2>
  <ul>
    <li>Keynotes</li>
    <li>Lightning Talks</li>
    <li>Poster Presentations</li>
    <li>Live Audience Poll</li>
  </ul>
</div>

<div class="section">
  <h2>Additional Information</h2>
  <ul>
    <li><strong>Duration:</strong> Full-day</li>
    <li><strong>Expected Participants:</strong> 100–150</li>
    <li><strong>Proceedings:</strong> CEUR-WS or arXiv</li>
    <li><strong>Diversity:</strong> Inclusive organizers, reviewers, speakers</li>
    <li><strong>Publicity:</strong> ML-News, Twitter, Hugging Face, LMSYS</li>
  </ul>
</div>

 <h2>References</h2>
  <ul>
    <li>Agrawal, A. et al. 2015. VQA: Visual Question Answering Dataset. <a href="https://visualqa.org/download.html">visualqa.org</a></li>
    <li>AILab-CVC. 2024. SEED-Bench. <a href="https://github.com/AILab-CVC/SEED-Bench">github.com/AILab-CVC/SEED-Bench</a></li>
    <li>Brockman, G. et al. 2016. OpenAI Gym. <em>arXiv:1606.01540</em></li>
    <li>C3.ai. 2023. C3 Generative AI. <a href="https://c3.ai/c3-generative-ai-getting-the-most-out-of-enterprise-data/">c3.ai</a></li>
    <li>Chalkidis, I. et al. 2022. LexGLUE. ACL 60, 4310–4330.</li>
    <li>Chen, M. et al. 2021. Evaluating Code LLMs. <em>arXiv:2107.03374</em></li>
    <li>Stanford CRFM. 2022. HELM. <a href="https://crfm.stanford.edu/helm/">crfm.stanford.edu/helm</a></li>
    <li>Goyal, Y. et al. 2020. TextVQA. <a href="https://textvqa.org/">textvqa.org</a></li>
    <li>Guss, W. H. et al. 2019. MineRL. NeurIPS Comp Track.</li>
    <li>Hendrycks, D. et al. 2021. APPS. <a href="https://github.com/hendrycks/apps">github.com/hendrycks/apps</a></li>
    <li>Jha, S. et al. 2025. ITBench. <a href="https://github.com/itbench-hub/ITBench">github.com/itbench-hub/ITBench</a></li>
    <li>Jin, D. et al. 2020. MedQA. <a href="https://github.com/jind11/MedQA">github.com/jind11/MedQA</a></li>
    <li>Kaggle Team. 2025. Kaggle Benchmarks. <a href="https://www.kaggle.com/blog/announcing-kaggle-benchmarks">kaggle.com</a></li>
    <li>Kang Lab, UIUC. 2024. CVE-Bench. <a href="https://github.com/uiuc-kang-lab/cve-bench">github.com/uiuc-kang-lab/cve-bench</a></li>
    <li>Kimura, D. et al. 2023. NSA@IJCAI. <a href="https://nsa-wksp.github.io/">nsa-wksp.github.io</a></li>
    <li>Li, J. et al. 2023. BIRD-SQL. NeurIPS 36: 42330–42357.</li>
    <li>Liang, P. et al. 2023. HELM. TMLR.</li>
    <li>Liu, X. et al. 2023. AgentBench. <a href="https://github.com/THUDM/AgentBench">github.com/THUDM/AgentBench</a></li>
    <li>Makondo, N. et al. 2025a. RA@DLI. <a href="https://sites.google.com/view/robot-learning-for-africa-2025/home">sites.google.com</a></li>
    <li>Makondo, N. et al. 2025b. IndabaX. <a href="https://indabax.co.za/">indabax.co.za</a></li>
    <li>Makondo, N. et al. 2025c. ICRA@40. <a href="https://icra40.ieee.org/icra-2024/icra40-africa/">ieee.org</a></li>
    <li>Makondo, N. et al. 2025d. RAPDASA. <a href="https://site.rapdasa.org/annual-conference/">rapdasa.org</a></li>
    <li>Mnih, V. et al. 2015. Human-level control. <em>Nature 518(7540)</em>: 529–533.</li>
    <li>Myrzakhan, A. et al. 2024. Open-LLM-Leaderboard. <a href="https://github.com/VILA-Lab/Open-LLM-Leaderboard">github.com/VILA-Lab</a></li>
    <li>OpenAI. 2021. HumanEval. <a href="https://github.com/openai/human-eval">github.com/openai/human-eval</a></li>
    <li>OpenAI. 2024. PaperBench. <a href="https://openai.com/index/paperbench/">openai.com</a></li>
    <li>OpenBMB. 2023. ToolBench. <a href="https://github.com/OpenBMB/ToolBench">github.com/OpenBMB/ToolBench</a></li>
    <li>Patel, D. et al. 2025. AssetOpsBench. <a href="https://github.com/IBM/AssetOpsBench">github.com/IBM/AssetOpsBench</a></li>
    <li>Rajpurkar, P. et al. 2016. SQuAD. EMNLP.</li>
    <li>ST-WebAgentBench Team. 2025. <a href="https://sites.google.com/view/st-webagentbench/home">google.com/view/st-webagentbench</a></li>
    <li>Todorov, E. et al. 2012. MuJoCo. IROS.</li>
    <li>Wallace, E. et al. 2020. AdversarialQA. <a href="https://adversarialqa.github.io/">adversarialqa.github.io</a></li>
    <li>Wang, A. et al. 2019. SuperGLUE. <em>arXiv:1905.00537</em></li>
    <li>Wang, A. et al. 2018. GLUE. <em>arXiv:1804.07461</em></li>
    <li>WebArena. 2024. WebArena. <a href="https://github.com/web-arena-x/webarena">github.com/web-arena-x/webarena</a></li>
    <li>Ye, C. et al. 2024. MIRAI. <a href="https://github.com/yecchen/MIRAI">github.com/yecchen/MIRAI</a></li>
  </ul>

</body>
</html>
