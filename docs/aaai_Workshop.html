<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AgentWorkBench 2026</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; }
    h1, h2, h3 { color: #003366; }
    ul, ol { margin-left: 20px; }
    .authors, .section, .abstract, .topics, .committee { margin-bottom: 30px; }
    .small { font-size: 0.9em; color: #555; }
  </style>
</head>
<body>

<h1>AgentWorkBench 2026</h1>
<h2>Benchmarking AI Agents Across Modalities, Tasks, and Real-World Contexts</h2>

<div class="section" style="font-size: 0.85em; color: #555;">
  <h2 style="font-size: 1.5em; color: #0077aa;">
    This website contains the References related to the workshop, <em>AgentWorkBench 2026: Benchmarking AI Agents Across Modalities, Tasks, and Real-World Contexts</em>
  </h2>
</div>



 <h2>References</h2>
<ul>
  <li>Agrawal, A.; Lu, J.; Antol, S.; Mitchell, M.; Zitnick, C. L.; Batra, D.; and Parikh, D. 2015. VQA: Visual Question Answering Dataset. <a href="https://visualqa.org/download.html">https://visualqa.org/download.html</a>. Accessed: 2025-07-24.</li>
  <li>AILab-CVC. 2024. SEED-Bench: Benchmarking Multimodal LLMs with Synthetic and Editable Data. <a href="https://github.com/AILab-CVC/SEED-Bench">https://github.com/AILab-CVC/SEED-Bench</a>. Accessed: 2025-07-24.</li>
  <li>Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang, J.; and Zaremba, W. 2016. OpenAI Gym. ArXiv preprint arXiv:1606.01540.</li>
  <li>C3.ai. 2023. C3 Generative AI: Getting the Most Out of Enterprise Data. <a href="https://c3.ai/c3-generative-ai-getting-the-most-out-of-enterprise-data/">https://c3.ai/c3-generative-ai-getting-the-most-out-of-enterprise-data/</a>. Accessed: 2025-07-24.</li>
  <li>Chalkidis, I.; Jana, A.; Hartung, D.; Bommarito, M.; Androutsopoulos, I.; Katz, D.; and Aletras, N. 2022. LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Long Papers), 4310–4330. Accessed: 2025-07-24.</li>
  <li>Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.; Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; Ray, A.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf, H.; Sastry, G.; Mishkin, P.; Chan, B.; Gray, S.; Ryder, N.; Pavlov, M.; Power, A.; Kaiser, L.; Bavarian, M.; Winter, C.; Tillet, P.; Such, F.; Cummings, D.; Plappert, M.; Chantzis, F.; Barnes, E.; Herbert-Voss, A.; Guss, W.; Nichol, A.; Paino, A.; Tezak, N.; Tang, J.; Babuschkin, I.; Balaji, S.; Jain, S.; Saunders, W.; Hesse, C.; Carr, A. N.; Leike, J.; Achiam, J.; Misra, V.; Morikawa, E.; Radford, A.; Knight, M.; Brundage, M.; Murati, M.; Mayer, K.; Welinder, P.; McGrew, B.; Amodei, D.; McCandlish, S.; Sutskever, I.; and Zaremba, W. 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374.</li>
  <li>Stanford Center for Research on Foundation Models (CRFM). 2022. HELM: Holistic Evaluation of Language Models. <a href="https://crfm.stanford.edu/helm/">https://crfm.stanford.edu/helm/</a>. Accessed: 2025-07-24.</li>
  <li>Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.; and Parikh, D. 2020. TextVQA: Visual Question Answering on Textual Content in Images. <a href="https://textvqa.org/">https://textvqa.org/</a>. Accessed: 2025-07-24.</li>
  <li>Guss, W. H.; Codel, C.; Hofmann, K.; Houghton, B.; et al. 2019. The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors. In NeurIPS 2019 Competition and Demonstration Track.</li>
  <li>Hendrycks, D.; Basart, S.; Kadavath, S.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Coding Challenge Competence With APPS. <a href="https://github.com/hendrycks/apps">https://github.com/hendrycks/apps</a>. Accessed: 2025-07-24.</li>
  <li>Jha, S.; Arora, R.; Watanabe, Y.; et al. 2025. ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks. <a href="https://github.com/itbench-hub/ITBench">https://github.com/itbench-hub/ITBench</a>. Accessed: 2025-07-24.</li>
  <li>Jin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and Szolovits, P. 2020. MedQA: A Large-scale Open Domain Question Answering Dataset from Medical Exams. <a href="https://github.com/jind11/MedQA">https://github.com/jind11/MedQA</a>. Accessed: 2025-07-24.</li>
  <li>Jin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and Szolovits, P. 2020. MedQA: A Large-scale Open Domain. <a href="https://github.com/jind11/MedQA">https://github.com/jind11/MedQA</a>. Accessed: 2025-07-24.</li>
  <li>Kaggle Team. 2025. Announcing Kaggle Benchmarks. <a href="https://www.kaggle.com/blog/announcing-kaggle-benchmarks">https://www.kaggle.com/blog/announcing-kaggle-benchmarks</a>. Accessed: 2025-07-25.</li>
  <li>Kang Lab, UIUC. 2024. CVE-Bench: Benchmark for Causal and Counterfactual Evaluation of Vision-Language Models. <a href="https://github.com/uiuc-kang-lab/cve-bench">https://github.com/uiuc-kang-lab/cve-bench</a>. Accessed: 2025-07-25.</li>
  <li>Kimura, D.; Makondo, N.; James, S.; et al. 2023. NeuroSymbolic Agents @IJCAI. <a href="https://nsa-wksp.github.io/">https://nsa-wksp.github.io/</a>. Accessed: 2025-07-24.</li>
  <li>Li, J.; Hui, B.; Qu, G.; Yang, J.; Li, B.; Li, B.; Wang, B.; Qin, B.; Geng, R.; Huo, N.; et al. 2023. Can LLM already serve as a database interface? A big bench for large-scale database grounded text-to-sqls. <em>Advances in Neural Information Processing Systems</em>, 36: 42330–42357.</li>
  <li>Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; et al. 2023. Holistic Evaluation of Language Models. <em>Transactions on Machine Learning Research</em>. Introduces Stanford CRFM’s HELM, living benchmark accessed via GitHub and CRFM website, accessed July 24, 2025.</li>
  <li>Liu, X.; Yu, H.; Zhang, H.; Xu, Y.; Lei, X.; Lai, H.; Gu, Y.; Ding, H.; Men, K.; Yang, K.; Zhang, S.; Deng, Z.; et al. 2023. AgentBench: Evaluating Large Language Models as Agents. <a href="https://github.com/THUDM/AgentBench">https://github.com/THUDM/AgentBench</a>. Introduced in August 2023, accessed July 24, 2025.</li>
  <li>Makondo, N.; Zwane, S.; Love, T.; et al. 2025a. Robotics and Automation in Africa @Deep Learning Indaba. <a href="https://sites.google.com/view/robot-learning-for-africa-2025/home">https://sites.google.com/view/robot-learning-for-africa-2025/home</a>. Accessed: 2025-07-24.</li>
  <li>Makondo, N.; et al. 2025b. Deep Learning Indabax - South Africa. <a href="https://indabax.co.za/">https://indabax.co.za/</a>. Accessed: 2025-07-24.</li>
  <li>Makondo, N.; et al. 2025c. IEEE ICRA@40 - Africa satellite. <a href="https://icra40.ieee.org/icra-2024/icra40-africa/">https://icra40.ieee.org/icra-2024/icra40-africa/</a>. Accessed: 2025-07-24.</li>
  <li>Makondo, N.; et al. 2025d. International Joint Conference on RAPDASA, RobMech, PRASA and AMI. <a href="https://site.rapdasa.org/annual-conference/">https://site.rapdasa.org/annual-conference/</a>. Accessed: 2025-07-24.</li>
  <li>Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; et al. 2015. Human-level control through deep reinforcement learning. <em>Nature</em>, 518(7540): 529-533.</li>
  <li>Myrzakhan, A.; Bsharat, S. M.; and Shen, Z. 2024. Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena. <a href="https://github.com/VILA-Lab/Open-LLM-Leaderboard">https://github.com/VILA-Lab/Open-LLM-Leaderboard</a>. Associated with arXiv preprint arXiv:2406.07545, accessed July 24, 2025.</li>
  <li>OpenAI. 2021. HumanEval: A Benchmark for Evaluating the Functional Correctness of Code Generation Models. <a href="https://github.com/openai/human-eval">https://github.com/openai/human-eval</a>. Accessed: 2025-07-24.</li>
  <li>OpenAI. 2024. Introducing PaperBench: Evaluating Research Comprehension in AI Models. <a href="https://openai.com/index/paperbench/">https://openai.com/index/paperbench/</a>. Accessed: 2025-07-25.</li>


  <li>OpenBMB. 2023. ToolBench: Towards Empowering Large Language Models with Web-grounded Tool-use.
    <a href="https://github.com/OpenBMB/ToolBench">https://github.com/OpenBMB/ToolBench</a>. Accessed: 2025-07-24.
  </li>

  <li>Patel, D.; Lin, S.; Rayfield, J.; Zhou, N.; Vaculin, R.; Martinez, N.; O’Donncha, F.; and Kalagnanam, J. 2025.
    AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance.
    <a href="https://github.com/IBM/AssetOpsBench">https://github.com/IBM/AssetOpsBench</a>. Accessed: 2025-07-24.
  </li>

  <li>Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text.
    In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2383–2392.
  </li>

  <li>ST-WebAgentBench Team. 2025. ST-WebAgentBench: A Benchmark for Short-Term Web Agent Tasks.
    <a href="https://sites.google.com/view/st-webagentbench/home">https://sites.google.com/view/st-webagentbench/home</a>. Accessed: 2025-07-25.
  </li>

  <li>Todorov, E.; Erez, T.; and Tassa, Y. 2012. MuJoCo: A physics engine for model-based control.
    In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 5026–5033.
  </li>

  <li>Wallace, E.; Rodriguez, P.; Feng, S.; Yamada, I.; and Boyd-Graber, J. 2020. AdversarialQA: A Benchmark for Reading Comprehension under Attack.
    <a href="https://adversarialqa.github.io/">https://adversarialqa.github.io/</a>. Accessed: 2025-07-24.
  </li>

  <li>Wang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019.
    SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.
    <em>arXiv preprint arXiv:1905.00537</em>.
  </li>

  <li>Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2018.
    GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.
    <em>arXiv preprint arXiv:1804.07461</em>.
  </li>

  <li>web-arena x. 2024. WebArena: A Realistic Web Environment for Building Autonomous Agents.
    <a href="https://github.com/webarena-x/webarena">https://github.com/webarena-x/webarena</a>. Accessed: 2025-07-24.
  </li>

  <li>Ye, C.; Hu, Z.; Deng, Y.; Huang, Z.; Ma, M. D.; Zhu, Y.; and Wang, W. 2024. MIRAI: Evaluating LLM Agents for Event Forecasting.
    <a href="https://github.com/yecchen/MIRAI">https://github.com/yecchen/MIRAI</a>. Associated arXiv preprint 2407.01231, accessed July 24, 2025.
  </li>








</ul>



</body>
</html>
